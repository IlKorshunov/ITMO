{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Union\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "pathToText = '/home/kolya/text-IKorshunov/train.txt'\n",
    "EOS = \"<EOS>\"\n",
    "UNK = \"<UNK>\"\n",
    "EOW = \"</w>\"\n",
    "EOP = \"@@\"\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "UNK_ID = 0\n",
    "PAD_ID = 1\n",
    "EOS_ID = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SUB_RE = re.compile(r\"[^аеёиоуыэюя]*[аеёиоуыэюя]+[^аеёиоуыэюя]*\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "04569251",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"\".join(re.findall(SUB_RE, \"курица\")) == \"курица\"\n",
    "assert \"\".join(re.findall(SUB_RE, \"абрикос\")) == \"абрикос\"\n",
    "assert \"\".join(re.findall(SUB_RE, \"оранжерея\")) == \"оранжерея\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, data: Union[str, List[str]], num_merges, device, flag=False):\n",
    "        self.device = device\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = []\n",
    "        self.vocab: dict[str, int] = {UNK: UNK_ID, PAD: PAD_ID, EOS: EOS_ID}\n",
    "        self.words = data.split() if isinstance(data, str) else data\n",
    "        self.flag = flag\n",
    "        self._fit(self.words)\n",
    "        self.inv_vocab = {idx: tok for tok, idx in self.vocab.items()}\n",
    "\n",
    "    def _word2tuple(self, word):\n",
    "        if self.flag:\n",
    "            return tuple(SUB_RE.findall(word.lower()))\n",
    "        else:\n",
    "            return tuple(list(word) + [EOW])\n",
    "\n",
    "    @staticmethod\n",
    "    def _tuple2display(tokens):\n",
    "        out= []\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok == EOW: continue\n",
    "            last = (i == len(tokens) - 1) or (tokens[i + 1] == EOW)\n",
    "            out.append(tok if last else tok + EOP)\n",
    "        return out\n",
    "\n",
    "    def _get_pair_stats(self, corpus: Counter):\n",
    "        stats = Counter()\n",
    "        for word_tuple, freq in corpus.items():\n",
    "            for i in range(len(word_tuple) - 1):\n",
    "                pair = (word_tuple[i], word_tuple[i + 1])\n",
    "                stats[pair] += freq\n",
    "        return stats\n",
    "\n",
    "    def _merge_in_word(self, word, pair):\n",
    "        a, b = pair\n",
    "        out: List[str] = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == a and word[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(word[i])\n",
    "                i += 1\n",
    "        return tuple(out)\n",
    "\n",
    "    def _apply_merge(self, corpus, pair):\n",
    "        merged = Counter()\n",
    "        for word_tuple, freq in corpus.items(): merged[self._merge_in_word(word_tuple, pair)] += freq\n",
    "        return merged\n",
    "\n",
    "    def _fit(self, words):\n",
    "        corpus = Counter(self._word2tuple(w) for w in words)\n",
    "        for _ in range(self.num_merges):\n",
    "            stats = self._get_pair_stats(corpus)\n",
    "            if not stats: break\n",
    "            best_pair, _ = stats.most_common(1)[0]\n",
    "            corpus = self._apply_merge(corpus, best_pair)\n",
    "            self.merges.append(best_pair)\n",
    "        new_tokens = {tok for w in corpus for tok in w} - {UNK, PAD, EOS}\n",
    "        for tok in sorted(new_tokens):\n",
    "            if tok not in self.vocab:\n",
    "                self.vocab[tok] = len(self.vocab)\n",
    "\n",
    "    def encode(self, token: str):\n",
    "        if token == EOS:\n",
    "            ids = [self.vocab[EOS]]\n",
    "        elif token == PAD:\n",
    "            ids = [self.vocab[PAD]]\n",
    "        else:\n",
    "            tokens = self._word2tuple(token)\n",
    "            for pair in self.merges: tokens = self._merge_in_word(tokens, pair)\n",
    "            raw_tokens = [t for t in tokens if t != EOW]\n",
    "            ids = [self.vocab.get(t, self.vocab[UNK]) for t in raw_tokens]\n",
    "        return torch.tensor(ids, device=self.device)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self._tuple2display(tuple(self.inv_vocab.get(i, UNK) for i in ids)))\n",
    "\n",
    "    def humanDecode(self, ids):\n",
    "        return \"\".join(self.inv_vocab.get(i, UNK) for i in ids if i != PAD_ID).replace(EOW, \" \").replace(EOS, \".\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "962ebacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(rf\"{EOS}|{UNK}|[А-Яа-яЁё]+|[.!?]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "21014ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, path2text: str, num_merges: int, device: torch.device):\n",
    "        self.path2text = path2text\n",
    "        self.num_merges = num_merges\n",
    "        self.device = device\n",
    "        self._read_file()\n",
    "        self._prepare_sentences()\n",
    "        self._make_word_list()\n",
    "\n",
    "        self.tokenizer = BPETokenizer(self.words, self.num_merges, device=self.device)\n",
    "        self.encoded_sentences = [[self.tokenizer.encode(tok).to(self.device) for tok in TOKEN_RE.findall(sent)] for sent in self.sentences]\n",
    "\n",
    "    def _read_file(self):\n",
    "        with open(self.path2text, \"r\", encoding=\"utf-8\") as f: self.text = f.read()\n",
    "\n",
    "    def _prepare_sentences(self):\n",
    "        tmp = re.sub(r\"\\s*([.!?])\\s*\", rf\" \\1 {EOS} \", self.text.strip())\n",
    "        raw = [s.strip() for s in tmp.split(EOS) if s.strip()]\n",
    "        self.sentences = [f\"{' '.join(TOKEN_RE.findall(s))} {EOS}\" for s in raw]\n",
    "\n",
    "    def _make_word_list(self):\n",
    "        self.words = [tok for sent in self.sentences for tok in TOKEN_RE.findall(sent) if tok != EOS]\n",
    "\n",
    "    def getPartText(self, first: int = 6, last: int = 15):\n",
    "        for i, enc_sent in enumerate(self.encoded_sentences[first - 1:last], first): print(f\"Sentence {i}: {enc_sent}\\n\")\n",
    "\n",
    "    def gerEncodeText(self):\n",
    "        return self.encoded_sentences\n",
    "\n",
    "    def getText(self):\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1a7b9804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 6: [tensor([24], device='cuda:0'), tensor([42, 44, 88, 44, 63], device='cuda:0'), tensor([62, 74, 44, 42, 75], device='cuda:0'), tensor([88, 62, 35, 67], device='cuda:0'), tensor([ 8, 44, 52], device='cuda:0'), tensor([82, 86, 43, 53, 90, 60, 76, 53, 57], device='cuda:0'), tensor([90, 44, 57], device='cuda:0'), tensor([51, 45], device='cuda:0'), tensor([102,  35,  90], device='cuda:0'), tensor([ 22,  74,  52,  39,  80, 108,  93,  45], device='cuda:0'), tensor([83, 52, 66, 58, 74, 62, 53, 95], device='cuda:0'), tensor([39, 35, 90], device='cuda:0'), tensor([21, 68, 42, 53, 65], device='cuda:0'), tensor([ 43,  74,  38,  84, 106,  57], device='cuda:0'), tensor([62, 79], device='cuda:0'), tensor([ 82,  87, 113,  93,  46, 109], device='cuda:0'), tensor([ 23,  78,  55,  90, 114], device='cuda:0'), tensor([67], device='cuda:0'), tensor([ 38,  86,  42,  35, 100], device='cuda:0'), tensor([ 20,  44,  39, 107], device='cuda:0'), tensor([10, 43, 45], device='cuda:0'), tensor([62, 74, 51, 50], device='cuda:0'), tensor([ 38, 106,  95], device='cuda:0'), tensor([ 88,  43,  55,  53,  90, 109], device='cuda:0'), tensor([ 39, 107], device='cuda:0'), tensor([15, 60, 54], device='cuda:0'), tensor([38, 60, 53, 92, 37, 54], device='cuda:0'), tensor([62, 79], device='cuda:0'), tensor([102,  53,  93,  35,  93,  46, 109], device='cuda:0'), tensor([25, 35, 63], device='cuda:0'), tensor([68, 58, 77, 43, 36], device='cuda:0'), tensor([ 42,  96,  60, 113,  60], device='cuda:0'), tensor([54], device='cuda:0'), tensor([114], device='cuda:0'), tensor([20, 75], device='cuda:0'), tensor([39, 86, 43, 47], device='cuda:0'), tensor([90, 44, 39, 48], device='cuda:0'), tensor([ 43,  60, 114], device='cuda:0'), tensor([ 62,  47, 114], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 7: [tensor([24, 60, 96, 51, 53, 40], device='cuda:0'), tensor([ 81,  60,  53, 102,  71], device='cuda:0'), tensor([38, 61, 42, 74, 88, 43, 71], device='cuda:0'), tensor([11, 80, 42, 35, 62, 54], device='cuda:0'), tensor([51, 55], device='cuda:0'), tensor([44, 42, 75], device='cuda:0'), tensor([ 81,  44, 101], device='cuda:0'), tensor([11, 35, 39, 37], device='cuda:0'), tensor([93, 84, 54], device='cuda:0'), tensor([38, 37, 36], device='cuda:0'), tensor([44, 51, 44, 42, 78, 71], device='cuda:0'), tensor([16], device='cuda:0'), tensor([ 82,  88,  62,  81,  37,  90, 114], device='cuda:0'), tensor([ 66,  58,  74,  68, 101], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 8: [tensor([ 24,  96,  43, 108,  38,  36], device='cuda:0'), tensor([ 12,  39,  42,  47,  53, 114], device='cuda:0'), tensor([99, 85, 64, 55, 36], device='cuda:0'), tensor([24, 82, 48, 39, 36], device='cuda:0'), tensor([52, 36], device='cuda:0'), tensor([70, 63], device='cuda:0'), tensor([99, 78, 55, 36], device='cuda:0'), tensor([22, 81, 74, 63], device='cuda:0'), tensor([44, 45], device='cuda:0'), tensor([90, 62, 47, 55], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 9: [tensor([23, 44, 38, 47, 74, 59], device='cuda:0'), tensor([ 38, 106,  60], device='cuda:0'), tensor([86, 52, 76], device='cuda:0'), tensor([71], device='cuda:0'), tensor([62, 55], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 10: [tensor([30, 93, 74, 38], device='cuda:0'), tensor([69], device='cuda:0'), tensor([ 53,  52,  62,  96, 102,  55,  74,  90, 109], device='cuda:0'), tensor([ 43,  53,  93, 114], device='cuda:0'), tensor([ 26, 102,  55], device='cuda:0'), tensor([44, 42, 75], device='cuda:0'), tensor([39, 90, 44, 62, 97], device='cuda:0'), tensor([103,  96,  93, 114], device='cuda:0'), tensor([20, 45], device='cuda:0'), tensor([ 43,  74,  58,  96, 102,  37], device='cuda:0'), tensor([ 62,  74,  85,  60, 108, 112], device='cuda:0'), tensor([92, 88, 42, 79], device='cuda:0'), tensor([24, 60, 44, 42, 58, 36], device='cuda:0'), tensor([52, 36], device='cuda:0'), tensor([103,  37,  74,  92,  54], device='cuda:0'), tensor([38, 85, 64, 55], device='cuda:0'), tensor([16], device='cuda:0'), tensor([40], device='cuda:0'), tensor([18, 49, 70, 57], device='cuda:0'), tensor([90, 35, 43], device='cuda:0'), tensor([ 42,  96,  60, 113,  95], device='cuda:0'), tensor([41, 43, 55], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 11: [tensor([17, 77, 43, 36], device='cuda:0'), tensor([51, 45], device='cuda:0'), tensor([111,  64,  74,  92,  54], device='cuda:0'), tensor([ 62, 113,  93,  44,  51,  64,  79], device='cuda:0'), tensor([ 22,  87, 103,  60,  36], device='cuda:0'), tensor([ 12,  39,  42,  47,  53, 112], device='cuda:0'), tensor([83, 84, 36], device='cuda:0'), tensor([22, 74, 84, 36], device='cuda:0'), tensor([66, 43, 44, 51, 43], device='cuda:0'), tensor([54], device='cuda:0'), tensor([42, 89, 92, 54], device='cuda:0'), tensor([68, 51, 64, 79], device='cuda:0'), tensor([82, 88, 42, 64, 37, 54], device='cuda:0'), tensor([90, 75], device='cuda:0'), tensor([43, 41, 84, 36], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 12: [tensor([ 9, 74, 94], device='cuda:0'), tensor([62, 79], device='cuda:0'), tensor([21, 68, 42, 53, 65], device='cuda:0'), tensor([67], device='cuda:0'), tensor([90, 41, 38, 78, 45], device='cuda:0'), tensor([21, 92, 87, 51, 47], device='cuda:0'), tensor([82, 75], device='cuda:0'), tensor([83, 90, 60, 44, 43, 68, 57], device='cuda:0'), tensor([62, 78, 45], device='cuda:0'), tensor([17, 35, 59], device='cuda:0'), tensor([60, 74, 64, 43, 74, 64, 91, 53, 57], device='cuda:0'), tensor([78, 50], device='cuda:0'), tensor([16], device='cuda:0'), tensor([ 66,  58,  74,  68, 101], device='cuda:0'), tensor([96, 39, 53, 43, 46], device='cuda:0'), tensor([90, 39, 50], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 13: [tensor([21, 65], device='cuda:0'), tensor([82, 75], device='cuda:0'), tensor([ 98,  85,  64, 101,  96,  52,  91,  54], device='cuda:0'), tensor([ 90,  76,  48, 103,  47,  71], device='cuda:0'), tensor([19, 77], device='cuda:0'), tensor([ 53,  52, 105, 113,  90,  64, 113,  93, 108,  90, 114],\n",
      "       device='cuda:0'), tensor([54], device='cuda:0'), tensor([82, 53, 90, 37], device='cuda:0'), tensor([18, 44, 42, 58, 75], device='cuda:0'), tensor([62, 35, 52, 96, 84, 58, 97], device='cuda:0'), tensor([ 93,  35,  64, 101,  44,  39,  37], device='cuda:0'), tensor([16], device='cuda:0'), tensor([ 58,  61,  64, 113,  60,  90, 114], device='cuda:0'), tensor([68, 82, 87, 64, 96, 51, 43, 47, 71], device='cuda:0'), tensor([30, 44, 42, 75], device='cuda:0'), tensor([51], device='cuda:0'), tensor([39, 35, 63], device='cuda:0'), tensor([ 38,  80, 108, 103,  45], device='cuda:0'), tensor([6], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 14: [tensor([24, 39, 50], device='cuda:0'), tensor([ 86, 103,  55], device='cuda:0'), tensor([30, 93, 75], device='cuda:0'), tensor([74, 65], device='cuda:0'), tensor([96, 62, 47], device='cuda:0'), tensor([54], device='cuda:0'), tensor([ 74, 102,  47, 109], device='cuda:0'), tensor([62, 55], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n",
      "Sentence 15: [tensor([ 19, 107], device='cuda:0'), tensor([39, 90, 45], device='cuda:0'), tensor([ 96, 102,  55,  53,  90, 109], device='cuda:0'), tensor([83, 68, 62, 64, 77, 97], device='cuda:0'), tensor([30, 44, 62, 97], device='cuda:0'), tensor([ 70,  38,  96,  43, 109], device='cuda:0'), tensor([54], device='cuda:0'), tensor([58, 35, 59], device='cuda:0'), tensor([ 70,  38,  96,  43, 109], device='cuda:0'), tensor([25, 35, 59], device='cuda:0'), tensor([ 41,  90,  82,  53,  93,  35,  64, 108,  44,  63], device='cuda:0'), tensor([90, 61, 39, 36], device='cuda:0'), tensor([38, 77, 97], device='cuda:0'), tensor([26], device='cuda:0'), tensor([66, 90], device='cuda:0'), tensor([68, 62, 96, 43, 84, 47, 75], device='cuda:0'), tensor([38, 60, 44, 90, 64, 96, 95], device='cuda:0'), tensor([4], device='cuda:0'), tensor([2], device='cuda:0')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processor = Preprocessing(pathToText, 50, DEVICE)\n",
    "processor.getPartText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(TorchDataset):\n",
    "    def __init__(self, encoded_text: List[List[List[int]]], seq_len: int, device: torch.device):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        self.pad_id = PAD_ID\n",
    "        self.all_ids: List[Tuple[List[int], List[int]]] = []\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.all_ids.clear()\n",
    "        for sent in self.encoded_text:\n",
    "            flattened = [tok for sub in sent for tok in sub]\n",
    "            for start in range(0, len(flattened), self.seq_len):\n",
    "                window = flattened[start : start + self.seq_len + 1]\n",
    "                if len(window) < self.seq_len + 1: window += [self.pad_id] * (self.seq_len + 1 - len(window))\n",
    "                x_ids = window[: self.seq_len]\n",
    "                y_ids = window[1 : self.seq_len + 1]\n",
    "                self.all_ids.append((x_ids, y_ids))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.all_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_ids, y_ids = self.all_ids[idx]\n",
    "        return torch.tensor(x_ids), torch.tensor(y_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4d5c1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, device, dropout):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID).to(self.device)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True, dropout=dropout).to(self.device)\n",
    "        self.dropout = nn.Dropout(dropout).to(self.device)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size).to(self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: Optional[tuple] = None):\n",
    "        x = x.to(self.device)\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.lstm(emb, hidden)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "    def fit(self, loader, epochs, lr):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "        for curEpoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total = 0.0\n",
    "            hidden= None\n",
    "            for i, (x, y) in enumerate(loader, 1):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device) \n",
    "                if hidden is not None and hidden[0].size(1) != x.size(0): hidden = None\n",
    "                if hidden is not None: hidden = tuple(h.detach() for h in hidden)\n",
    "                logits, hidden = self.forward(x, hidden)\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total += loss.item()\n",
    "            print(f\"[epoch {curEpoch}/{epochs}] loss={total/len(loader):.4f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _sample_next(self, logits: torch.Tensor, past, temperature, top_k, top_p, rep_penalty):\n",
    "        logits = logits.clone().to(self.device)/temperature\n",
    "        if rep_penalty != 1.0:\n",
    "            for t in past: logits[..., t]/=rep_penalty\n",
    "        if top_k > 0:\n",
    "            kth_vals, _ = logits.topk(top_k)\n",
    "            logits = torch.where(logits < kth_vals[:, -1].unsqueeze(1), torch.full_like(logits, -float('Inf')), logits)\n",
    "        # if 0 < top_p < 1.0: \n",
    "        #     probs_sorted, idx_sorted = F.softmax(logits, dim=-1).sort(descending=True)\n",
    "        #     cum_probs = probs_sorted.cumsum(dim=-1)\n",
    "        #     keep = cum_probs <= top_p\n",
    "        #     keep[:, 0] = True\n",
    "        #     keep_mask = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        #     batch_idx = torch.arange(logits.size(0), device=logits.device).unsqueeze(1)\n",
    "        #     keep_mask[batch_idx, idx_sorted] = keep\n",
    "        #     logits.masked_fill_(~keep_mask, float('-inf'))\n",
    "            \n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        return torch.multinomial(probs, 1).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start: torch.LongTensor, max_len, device, temperature, top_k, top_p, rep_penalty):\n",
    "        self.eval()\n",
    "        seq = start.to(device)\n",
    "        hidden = None\n",
    "        out = [seq]\n",
    "        last = []\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = self.forward(seq, hidden)\n",
    "            nextTok = self._sample_next(logits[:, -1, :], last, temperature, top_k, top_p, rep_penalty)\n",
    "            out.append(nextTok)\n",
    "            seq = nextTok\n",
    "            last.append(nextTok.item())\n",
    "        return torch.cat(out, dim = 1)\n",
    "\n",
    "    def generate_greedy(self, start, max_len, device): return self.generate(start, max_len, device, top_k = 1, temperature = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7a9fc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = Preprocessing(path2text=pathToText, num_merges=1000, device=DEVICE)\n",
    "dataset = MyDataset(proc.gerEncodeText(), seq_len=150, device=DEVICE)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3385a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/3] loss=6.9613\n",
      "[epoch 2/3] loss=6.9349\n",
      "[epoch 3/3] loss=6.8490\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(len(proc.tokenizer.vocab), 128, 256, num_layers=3, device=DEVICE, dropout=0.3)\n",
    "model.fit(loader, epochs=3, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e5365530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Излиться тпризи поя ла. .И не ен! дв\n"
     ]
    }
   ],
   "source": [
    "ids= model.generate(proc.tokenizer.encode(\"Излиться\").to(DEVICE).unsqueeze(0), max_len=15, device=DEVICE, temperature=1.5, top_k=20, top_p=0, rep_penalty=20)[0].tolist()\n",
    "print(proc.tokenizer.humanDecode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    def __init__(self, vocab_size: int, device: torch.device):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.freqs = torch.zeros((vocab_size, vocab_size), device=self.device)\n",
    "        self.probs = None\n",
    "\n",
    "    def getProbs(self):\n",
    "        denom = self.freqs.sum(dim=1, keepdim=True).clone()\n",
    "        denom[denom == 0] = 1\n",
    "        self.probs = self.freqs/denom  \n",
    "\n",
    "    def fit(self, loader):\n",
    "        for x, y in loader:\n",
    "            for prev_tok, nextTok in zip(x.reshape(-1).tolist(), y.reshape(-1).tolist()): self.freqs[prev_tok, nextTok] += 1.0\n",
    "        self.getProbs()\n",
    "        \n",
    "    def generate(self, startIdx, max_len):\n",
    "        if isinstance(startIdx, torch.Tensor):\n",
    "            seq = startIdx.tolist()\n",
    "        else:\n",
    "            seq = list(startIdx)\n",
    "            \n",
    "        for _ in range(max_len):\n",
    "            seq.append(int(torch.multinomial(self.probs[seq[-1]], num_samples=1).item()))\n",
    "            \n",
    "        return torch.tensor([seq], device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "39a93e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Онегин мой зам Погиным И одна время года ! .\n"
     ]
    }
   ],
   "source": [
    "markov = MarkovModel(vocab_size=len(proc.tokenizer.vocab), device=DEVICE)\n",
    "markov.fit(loader)\n",
    "\n",
    "print(proc.tokenizer.humanDecode(markov.generate(proc.tokenizer.encode(\"Онегин\").tolist(), max_len=30)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
